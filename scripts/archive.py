#!/usr/bin/env python3
"""
æ•°æ®å½’æ¡£è„šæœ¬
åŠŸèƒ½ï¼šç®¡ç†å†å²æ•°æ®ï¼Œæ”¯æŒæ£€ç´¢å’Œå¤‡ä»½
"""

import os
import sys
import json
import gzip
import shutil
from datetime import datetime, timedelta
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import yaml


class DataArchiver:
    """æ•°æ®å½’æ¡£å™¨"""
    
    def __init__(self, config_path=None):
        """åˆå§‹åŒ–"""
        if config_path is None:
            script_dir = Path(__file__).parent.parent
            config_path = script_dir / "config.yaml"
        
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        self.raw_data_dir = self.config['storage']['raw_data_dir']
        self.processed_data_dir = self.config['storage']['processed_data_dir']
        self.backup_dir = self.config['storage']['backup_dir']
        self.max_history_days = self.config['storage']['max_history_days']
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(self.backup_dir, exist_ok=True)
    
    def list_files(self, data_type='all'):
        """åˆ—å‡ºæ•°æ®æ–‡ä»¶"""
        files = []
        
        if data_type in ['all', 'raw']:
            raw_files = sorted([
                {'path': os.path.join(self.raw_data_dir, f), 'type': 'raw', 'date': f.split('_')[1]}
                for f in os.listdir(self.raw_data_dir) if f.startswith('raw_')
            ])
            files.extend(raw_files)
        
        if data_type in ['all', 'processed']:
            processed_files = sorted([
                {'path': os.path.join(self.processed_data_dir, f), 'type': 'processed', 'date': f.split('_')[1]}
                for f in os.listdir(self.processed_data_dir) if f.startswith('processed_')
            ], key=lambda x: x['date'], reverse=True)
            files.extend(processed_files)
        
        if data_type in ['all', 'published']:
            published_files = sorted([
                {'path': os.path.join(self.published_data_dir, f), 'type': 'published', 'date': f.split('_')[2].replace('.md', '')}
                for f in os.listdir(self.published_data_dir) if f.startswith('morning_news_')
            ], key=lambda x: x['date'], reverse=True)
            files.extend(published_files)
        
        return files
    
    def get_file_info(self, filepath):
        """è·å–æ–‡ä»¶ä¿¡æ¯"""
        if not os.path.exists(filepath):
            return None
        
        stat = os.stat(filepath)
        return {
            'path': filepath,
            'size': stat.st_size,
            'modified': datetime.fromtimestamp(stat.st_mtime).isoformat(),
            'created': datetime.fromtimestamp(stat.st_ctime).isoformat()
        }
    
    def backup(self, days=7):
        """å¤‡ä»½æ•°æ®"""
        print("\n" + "=" * 60)
        print("ğŸ’¾ æ•°æ®å¤‡ä»½")
        print("=" * 60)
        
        # å¤‡ä»½ç›®å½•ä»¥æ—¥æœŸå‘½å
        backup_date = datetime.now().strftime('%Y%m%d')
        backup_folder = os.path.join(self.backup_dir, backup_date)
        os.makedirs(backup_folder, exist_ok=True)
        
        files_to_backup = self.list_files()
        backed_up = 0
        
        for file_info in files_to_backup:
            filepath = file_info['path']
            if not os.path.exists(filepath):
                continue
            
            # è®¡ç®—æ–‡ä»¶ä¿®æ”¹æ—¥æœŸ
            file_mtime = datetime.fromtimestamp(os.path.getmtime(filepath))
            if datetime.now() - file_mtime > timedelta(days=days):
                continue
            
            # å¤åˆ¶æ–‡ä»¶
            relative_path = os.path.relpath(filepath)
            dest_path = os.path.join(backup_folder, relative_path)
            
            os.makedirs(os.path.dirname(dest_path), exist_ok=True)
            shutil.copy2(filepath, dest_path)
            backed_up += 1
        
        print(f"âœ… å·²å¤‡ä»½ {backed_up} ä¸ªæ–‡ä»¶åˆ°: {backup_folder}")
        return backup_folder
    
    def cleanup(self, days=None):
        """æ¸…ç†æ—§æ•°æ®"""
        if days is None:
            days = self.max_history_days
        
        print("\nğŸ§¹ æ¸…ç†æ—§æ•°æ®...")
        
        cutoff_date = datetime.now() - timedelta(days=days)
        cleaned = 0
        
        for data_type in ['raw', 'processed']:
            dir_path = self.raw_data_dir if data_type == 'raw' else self.processed_data_dir
            
            for filename in os.listdir(dir_path):
                if not filename.startswith(data_type + '_'):
                    continue
                
                filepath = os.path.join(dir_path, filename)
                file_date = datetime.strptime(filename.split('_')[1], '%Y%m%d_%H%M%S')
                
                if file_date < cutoff_date:
                    os.remove(filepath)
                    cleaned += 1
        
        print(f"âœ… å·²æ¸…ç† {cleaned} ä¸ªæ—§æ–‡ä»¶")
        return cleaned
    
    def search(self, keyword, data_type='all', limit=10):
        """æœç´¢æ•°æ®"""
        print(f"\nğŸ” æœç´¢å…³é”®è¯: {keyword}")
        
        results = []
        files = self.list_files(data_type)
        
        for file_info in files:
            filepath = file_info['path']
            
            if filepath.endswith('.json'):
                with open(filepath, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # æœç´¢processedæ•°æ®
                if 'articles' in data:
                    for article in data['articles']:
                        text = f"{article.get('title', '')} {article.get('summary', '')}"
                        if keyword.lower() in text.lower():
                            results.append({
                                'type': 'article',
                                'title': article.get('title', ''),
                                'summary': article.get('summary', '')[:200],
                                'source': article.get('source', ''),
                                'date': file_info['date'],
                                'link': article.get('link', '')
                            })
            
            if len(results) >= limit:
                break
        
        print(f"âœ… æ‰¾åˆ° {len(results)} æ¡ç»“æœ")
        return results
    
    def get_stats(self):
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'raw_files': 0,
            'processed_files': 0,
            'published_files': 0,
            'total_articles': 0,
            'date_range': {'start': None, 'end': None}
        }
        
        dates = []
        
        # ç»Ÿè®¡rawæ–‡ä»¶
        for f in os.listdir(self.raw_data_dir):
            if f.startswith('raw_'):
                stats['raw_files'] += 1
                date_str = f.split('_')[1]
                dates.append(date_str)
        
        # ç»Ÿè®¡processedæ–‡ä»¶
        for f in os.listdir(self.processed_data_dir):
            if f.startswith('processed_'):
                stats['processed_files'] += 1
                date_str = f.split('_')[1]
                dates.append(date_str)
        
        # ç»Ÿè®¡publishedæ–‡ä»¶
        published_dir = self.config['morning_news']['output_dir']
        for f in os.listdir(published_dir):
            if f.startswith('morning_news_'):
                stats['published_files'] += 1
                date_str = f.split('_')[2].replace('.md', '')
                dates.append(date_str)
        
        # ç»Ÿè®¡æ–‡ç« æ•°
        for f in os.listdir(self.processed_data_dir):
            if f.startswith('processed_'):
                filepath = os.path.join(self.processed_data_dir, f)
                with open(filepath, 'r', encoding='utf-8') as file:
                    data = json.load(file)
                    stats['total_articles'] += data.get('total_articles', 0)
        
        if dates:
            dates.sort()
            stats['date_range']['start'] = dates[0]
            stats['date_range']['end'] = dates[-1]
        
        return stats
    
    def list_by_date(self, date_str=None):
        """æŒ‰æ—¥æœŸåˆ—å‡ºæ–‡ä»¶"""
        if not date_str:
            date_str = datetime.now().strftime('%Y%m%d')
        
        print(f"\nğŸ“… {date_str} çš„æ–‡ä»¶:")
        
        files = self.list_files()
        date_files = [f for f in files if date_str in f['date']]
        
        for f in date_files:
            info = self.get_file_info(f['path'])
            if info:
                size_kb = info['size'] / 1024
                print(f"  [{f['type']}] {os.path.basename(f['path'])} ({size_kb:.1f} KB)")
        
        return date_files


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ’¾ æ•°æ®å½’æ¡£ç®¡ç†å™¨")
    print("=" * 60)
    
    archiver = DataArchiver()
    
    # æ˜¾ç¤ºç»Ÿè®¡
    stats = archiver.get_stats()
    print("\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
    print(f"  åŸå§‹æ•°æ®æ–‡ä»¶: {stats['raw_files']}")
    print(f"  å¤„ç†åæ–‡ä»¶: {stats['processed_files']}")
    print(f"  å‘å¸ƒçš„æ—©æŠ¥: {stats['published_files']}")
    print(f"  æ€»æ–‡ç« æ•°: {stats['total_articles']}")
    if stats['date_range']['start']:
        print(f"  æ—¥æœŸèŒƒå›´: {stats['date_range']['start']} ~ {stats['date_range']['end']}")
    
    # åˆ—å‡ºä»Šå¤©çš„æ–‡ä»¶
    archiver.list_by_date()


if __name__ == '__main__':
    main()
