#!/usr/bin/env python3
"""
RSSé‡‡é›†è„šæœ¬
åŠŸèƒ½ï¼šä»å¤šä¸ªRSSæºé‡‡é›†æœ€æ–°èµ„è®¯
"""

import feedparser
import requests
import yaml
import json
import os
import sys
from datetime import datetime
from dateutil import parser as date_parser
from urllib.parse import urlparse
from pathlib import Path
import time
import urllib3
import ssl

# ç¦ç”¨SSLè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# åˆ›å»ºä¸éªŒè¯SSLçš„ä¸Šä¸‹æ–‡
ssl_context = ssl.create_default_context()
ssl_context.check_hostname = False
ssl_context.verify_mode = ssl.CERT_NONE

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

class RSSFetcher:
    """RSSé‡‡é›†å™¨"""
    
    def __init__(self, config_path=None):
        """åˆå§‹åŒ–åŠ è½½é…ç½®"""
        if config_path is None:
            script_dir = Path(__file__).parent.parent
            config_path = script_dir / "config.yaml"
        
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        self.sources = self.config['rss_sources']
        self.keywords = self.config['keywords']
        self.raw_data_dir = self.config['storage']['raw_data_dir']
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(self.raw_data_dir, exist_ok=True)
        
        # ä¼šè¯ç”¨äºHTTPè¯·æ±‚
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'DailyTechMorning/1.0'
        })
    
    def parse_date(self, date_str):
        """è§£ææ—¥æœŸå­—ç¬¦ä¸²"""
        if not date_str:
            return datetime.now().isoformat()
        
        try:
            dt = date_parser.parse(date_str)
            return dt.isoformat()
        except:
            return datetime.now().isoformat()
    
    def check_keywords(self, text, include=True):
        """æ£€æŸ¥å…³é”®è¯"""
        text_lower = text.lower()
        
        if include:
            # æ£€æŸ¥æ˜¯å¦åŒ…å«éœ€è¦çš„å…³é”®è¯
            for keyword in self.keywords.get('include', []):
                if keyword.lower() in text_lower:
                    return True
            return False
        else:
            # æ£€æŸ¥æ˜¯å¦åº”è¯¥æ’é™¤
            for keyword in self.keywords.get('exclude', []):
                if keyword.lower() in text_lower:
                    return False
            return True
    
    def clean_text(self, text):
        """æ¸…ç†æ–‡æœ¬"""
        if not text:
            return ""
        # ç§»é™¤å¤šä½™ç©ºç™½
        text = ' '.join(text.split())
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
        text = text.replace('\xa0', ' ')
        return text.strip()
    
    def fetch_single_source(self, source):
        """é‡‡é›†å•ä¸ªRSSæº"""
        name = source['name']
        url = source['url']
        category = source.get('category', 'tech')
        
        print(f"ğŸ“¥ é‡‡é›†: {name}...")
        
        try:
            # ä½¿ç”¨requestsè·å–RSSå†…å®¹ï¼ˆç¦ç”¨SSLéªŒè¯ï¼‰
            response = self.session.get(url, timeout=30, verify=False)
            response.raise_for_status()
            
            # ä½¿ç”¨feedparserè§£æå†…å®¹
            feed = feedparser.parse(response.content)
            
            if feed.bozo:
                print(f"  âš ï¸ è§£æè­¦å‘Š: {feed.bozo_exception}")
            
            entries = []
            for entry in feed.entries[:15]:  # å–å‰15æ¡
                # è·å–æ‘˜è¦
                summary = getattr(entry, 'summary', '') or getattr(entry, 'description', '')
                content = getattr(entry, 'content', [{}])[0].get('value', summary) if hasattr(entry, 'content') else summary
                
                # æ„å»ºæ–‡ç« ä¿¡æ¯
                article = {
                    'title': self.clean_text(entry.get('title', '')),
                    'link': entry.get('link', ''),
                    'summary': self.clean_text(summary[:500]),  # é™åˆ¶é•¿åº¦
                    'content': self.clean_text(content[:2000]),
                    'published': self.parse_date(entry.get('published', '')),
                    'updated': self.parse_date(entry.get('updated', '')),
                    'author': entry.get('author', ''),
                    'source': name,
                    'category': category,
                    'tags': [tag.term for tag in getattr(entry, 'tags', [])],
                    'fetched_at': datetime.now().isoformat()
                }
                
                # å…³é”®è¯è¿‡æ»¤
                text_to_check = f"{article['title']} {article['summary']}"
                if self.check_keywords(text_to_check, include=True):
                    if self.check_keywords(text_to_check, include=False):
                        entries.append(article)
            
            print(f"  âœ… è·å– {len(entries)} ç¯‡ç›¸å…³æ–‡ç« ")
            return {
                'source': name,
                'url': url,
                'category': category,
                'articles': entries,
                'fetched_at': datetime.now().isoformat()
            }
            
        except Exception as e:
            print(f"  âŒ é‡‡é›†å¤±è´¥: {e}")
            return {
                'source': name,
                'url': url,
                'category': category,
                'articles': [],
                'error': str(e),
                'fetched_at': datetime.now().isoformat()
            }
    
    def fetch_all(self):
        """é‡‡é›†æ‰€æœ‰RSSæº"""
        print(f"\nğŸ¤– å¼€å§‹é‡‡é›†RSSæº... ({datetime.now().strftime('%Y-%m-%d %H:%M:%S')})\n")
        
        results = []
        for source in self.sources:
            if source.get('enabled', True):
                result = self.fetch_single_source(source)
                results.append(result)
                time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
        
        # ä¿å­˜åŸå§‹æ•°æ®
        output_file = os.path.join(
            self.raw_data_dir,
            f"raw_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        )
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump({
                'fetched_at': datetime.now().isoformat(),
                'sources': results
            }, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ åŸå§‹æ•°æ®å·²ä¿å­˜: {output_file}")
        
        # ç»Ÿè®¡
        total_articles = sum(len(r.get('articles', [])) for r in results)
        print(f"\nğŸ“Š é‡‡é›†ç»Ÿè®¡:")
        print(f"   æ€»æºæ•°: {len(results)}")
        print(f"   æ€»æ–‡ç« : {total_articles}")
        
        return results
    
    def get_latest_raw(self):
        """è·å–æœ€æ–°çš„åŸå§‹æ•°æ®æ–‡ä»¶"""
        files = sorted([
            f for f in os.listdir(self.raw_data_dir) 
            if f.startswith('raw_') and f.endswith('.json')
        ])
        
        if not files:
            return None
        
        latest_file = os.path.join(self.raw_data_dir, files[-1])
        with open(latest_file, 'r', encoding='utf-8') as f:
            return json.load(f)


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ¤– æ¯æ—¥AIç§‘æŠ€æ—©æŠ¥ - RSSé‡‡é›†å™¨")
    print("=" * 60)
    
    fetcher = RSSFetcher()
    results = fetcher.fetch_all()
    
    return results


if __name__ == '__main__':
    main()
