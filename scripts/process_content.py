#!/usr/bin/env python3
"""
å†…å®¹å¤„ç†è„šæœ¬
åŠŸèƒ½ï¼šæ•´åˆæ‘˜è¦ç”Ÿæˆå’Œå…³é”®è¯æå–
"""

import os
import sys
import json
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import yaml
from fetch_rss import RSSFetcher
from summarize import AISummarizer, clean_html_text
from extract_keywords import KeywordExtractor


class ContentProcessor:
    """å†…å®¹å¤„ç†å™¨"""
    
    def __init__(self, config_path='config.yaml'):
        """åˆå§‹åŒ–"""
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        self.raw_data_dir = self.config['storage']['raw_data_dir']
        self.processed_data_dir = self.config['storage']['processed_data_dir']
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(self.processed_data_dir, exist_ok=True)
        
        self.summarizer = AISummarizer(config_path)
        self.extractor = KeywordExtractor()
    
    def process(self, use_ai=False):
        """å¤„ç†å†…å®¹"""
        print("\n" + "=" * 60)
        print("ğŸ“Š å†…å®¹å¤„ç†å™¨")
        print("=" * 60)
        
        # è·å–æœ€æ–°çš„åŸå§‹æ•°æ®
        fetcher = RSSFetcher()
        raw_data = fetcher.get_latest_raw()
        
        if not raw_data:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°åŸå§‹æ•°æ®ï¼Œè¯·å…ˆè¿è¡Œ fetch_rss.py")
            return None
        
        # æ”¶é›†æ‰€æœ‰æ–‡ç« 
        all_articles = []
        for source in raw_data.get('sources', []):
            all_articles.extend(source.get('articles', []))
        
        # è·å–æœ€è¿‘å‡ å¤©æ—©æŠ¥ä¸­å·²å­˜åœ¨çš„é“¾æ¥ï¼ˆè·¨å¤©å»é‡ï¼‰
        published_dir = self.config['morning_news']['output_dir']
        seen_links = set()
        
        if os.path.exists(published_dir):
            import glob
            # è·å–æœ€è¿‘7å¤©çš„æ—©æŠ¥æ–‡ä»¶
            report_files = sorted(glob.glob(os.path.join(published_dir, 'morning_news_*.md')), reverse=True)[:7]
            for report_file in report_files:
                try:
                    with open(report_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                        # æå–æ‰€æœ‰é“¾æ¥
                        import re
                        links = re.findall(r'\[é˜…è¯»åŸæ–‡\]\((https?://[^\s)]+)\)', content)
                        seen_links.update(links)
                except Exception as e:
                    print(f"  âš ï¸ è¯»å–æ—©æŠ¥å¤±è´¥: {report_file}")
        
        print(f"\nğŸ“° å·²æœ‰ {len(seen_links)} ç¯‡æ–‡ç« åœ¨æœ€è¿‘7å¤©æ—©æŠ¥ä¸­")
        
        # å»é‡ï¼ˆåŸºäºé“¾æ¥ï¼‰
        unique_articles = []
        for article in all_articles:
            link = article.get('link', '')
            if link and link not in seen_links:
                seen_links.add(link)
                unique_articles.append(article)
            elif link:
                print(f"  ğŸ”„ è·³è¿‡é‡å¤: {article.get('title', '')[:50]}...")
        
        print(f"\nğŸ“° å»é‡åå…±æœ‰ {len(unique_articles)} ç¯‡æ–°æ–‡ç« ")
        
        # å¤„ç†æ¯ç¯‡æ–‡ç« 
        print("\nğŸ”„ å¼€å§‹å¤„ç†æ–‡ç« ...")
        processed_articles = []
        
        for i, article in enumerate(unique_articles):
            # AIæ‘˜è¦
            summary_result = self.summarizer.summarize_article(article, use_ai=use_ai)
            
            # å…³é”®è¯æå–
            keyword_result = self.extractor.process_article(article)
            
            # åˆå¹¶ç»“æœ
            processed = {
                **article,
                'short_summary': summary_result['short_summary'],
                'funny_title': summary_result['funny_title'],
                'tags': summary_result['tags'] + keyword_result['keywords'][:3],
                'topics': keyword_result['topics'],
                'image_suggestion': summary_result['image_suggestion'],
                'processed_at': datetime.now().isoformat()
            }
            
            processed_articles.append(processed)
            
            if (i + 1) % 20 == 0:
                print(f"  å·²å¤„ç† {i + 1}/{len(unique_articles)} ç¯‡")
        
        print(f"  âœ… å¤„ç†å®Œæˆ {len(processed_articles)} ç¯‡æ–‡ç« ")
        
        # ä¿å­˜å¤„ç†åçš„æ•°æ®
        output_file = os.path.join(
            self.processed_data_dir,
            f"processed_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        )
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump({
                'processed_at': datetime.now().isoformat(),
                'total_articles': len(processed_articles),
                'articles': processed_articles
            }, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ å¤„ç†åæ•°æ®å·²ä¿å­˜: {output_file}")
        
        # ç»Ÿè®¡
        all_tags = []
        all_topics = []
        for article in processed_articles:
            all_tags.extend(article.get('tags', []))
            all_topics.extend(article.get('topics', []))
        
        from collections import Counter
        tag_stats = Counter(all_tags).most_common(10)
        topic_stats = Counter(all_topics).most_common(5)
        
        print(f"\nğŸ“Š ç»Ÿè®¡:")
        print(f"   çƒ­é—¨æ ‡ç­¾: {tag_stats[:5]}")
        print(f"   ä¸»é¢˜åˆ†å¸ƒ: {topic_stats}")
        
        return processed_articles
    
    def generate_enhanced_news(self, articles):
        """ç”Ÿæˆå¢å¼ºç‰ˆæ—©æŠ¥"""
        print("\nğŸ“ ç”Ÿæˆå¢å¼ºç‰ˆæ—©æŠ¥...")
        
        # æŒ‰ä¸»é¢˜åˆ†ç±»
        categorized = {
            'AIå‰æ²¿': [],
            'ç§‘æŠ€åˆ›æŠ•': [],
            'å¼€æºåŠ¨æ€': [],
            'å·¥å…·æ¨è': [],
            'å…¶ä»–': []
        }
        
        for article in articles:
            topics = article.get('topics', [])
            funny_title = article.get('funny_title', article.get('title', ''))
            short_summary = clean_html_text(article.get('short_summary', article.get('summary', '')))
            
            item = {
                'title': funny_title,
                'summary': short_summary,
                'source': article.get('source', ''),
                'link': article.get('link', ''),
                'tags': article.get('tags', [])[:3],
                'image': article.get('image_suggestion', '')
            }
            
            if 'AI/æœºå™¨å­¦ä¹ ' in topics:
                categorized['AIå‰æ²¿'].append(item)
            elif 'ç§‘æŠ€åˆ›æŠ•' in topics:
                categorized['ç§‘æŠ€åˆ›æŠ•'].append(item)
            elif 'å¼€æº' in topics:
                categorized['å¼€æºåŠ¨æ€'].append(item)
            elif 'å¼€å‘å·¥å…·' in topics:
                categorized['å·¥å…·æ¨è'].append(item)
            else:
                categorized['å…¶ä»–'].append(item)
        
        # ç”ŸæˆMarkdown
        today = datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %A')
        
        md_content = f"""---
title: "ğŸ¤– AIç§‘æŠ€æ—©æŠ¥ - {today}"
date: {datetime.now().isoformat()}
slug: morning-news-{datetime.now().strftime('%Y-%m-%d')}
categories:
  - ç§‘æŠ€èµ„è®¯
  - AIå‰æ²¿
tags:
  - æ¯æ—¥ç®€æŠ¥
  - ç§‘æŠ€
  - AI
---

# ğŸ¤– AIç§‘æŠ€æ—©æŠ¥ - {today}

> æ¯å¤©æ—©ä¸Š7ç‚¹ï¼Œäº†è§£AIå’Œç§‘æŠ€åœˆçš„æ–°é²œäº‹ï¼

---

## ğŸ“Š ä»Šæ—¥æ¦‚è§ˆ

| ç»Ÿè®¡é¡¹ | æ•°é‡ |
|--------|------|
| ğŸ“ æ–‡ç« æ€»æ•° | {len(articles)} ç¯‡ |
| ğŸ”¥ AIå‰æ²¿ | {len(categorized['AIå‰æ²¿'])} ç¯‡ |
| ğŸ’° åˆ›æŠ•åŠ¨æ€ | {len(categorized['ç§‘æŠ€åˆ›æŠ•'])} ç¯‡ |
| ğŸ› ï¸ å·¥å…·æ¨è | {len(categorized['å·¥å…·æ¨è'])} ç¯‡ |

---
"""
        
        for category, items in categorized.items():
            if items:
                md_content += f"\n## ğŸ¯ {category}\n\n"
                for i, item in enumerate(items[:5], 1):
                    md_content += f"### {i}. {item['title']}\n\n"
                    md_content += f">{item['summary']}\n\n"
                    md_content += f"**æ ‡ç­¾**: {' '.join([f'`{t}`' for t in item['tags']])} Â· "
                    md_content += f"**æ¥æº**: {item['source']}\n\n"
                    md_content += f"[é˜…è¯»åŸæ–‡]({item['link']}) Â· "
                    md_content += f"![é…å›¾å»ºè®®]({item['image']})\n\n"
        
        md_content += f"""
---

## ğŸ’¡ ä»Šæ—¥é‡‘å¥

> åœ¨AIæ—¶ä»£ï¼Œæœ€å¥½çš„æŠ•èµ„æ˜¯å­¦ä¹ æœ¬èº«ã€‚

---

> ğŸ“¢ **å…³æ³¨æˆ‘ä»¬**ï¼Œè·å–æ¯æ—¥æœ€æ–°ç§‘æŠ€èµ„è®¯ï¼
> 
> Powered by [Daily Tech Morning](https://github.com/EvilJul/daily-tech-morning)
"""
        
        # ä¿å­˜
        output_file = os.path.join(
            self.processed_data_dir,
            f"enhanced_morning_news_{datetime.now().strftime('%Y%m%d')}.md"
        )
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(md_content)
        
        print(f"âœ… å¢å¼ºç‰ˆæ—©æŠ¥å·²ç”Ÿæˆ: {output_file}")
        return output_file


def main():
    """ä¸»å‡½æ•°"""
    use_ai = '--ai' in sys.argv
    
    processor = ContentProcessor()
    articles = processor.process(use_ai=use_ai)
    
    if articles:
        processor.generate_enhanced_news(articles)


if __name__ == '__main__':
    main()
